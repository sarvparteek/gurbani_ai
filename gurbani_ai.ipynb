{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40dd71-aab2-4062-9548-6707f13332a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SikhiToTheMax/Khalis libraries\n",
    "import banidb\n",
    "from anvaad_py import firstLetters\n",
    "\n",
    "import requests # to get raags\n",
    "from sentence_transformers import SentenceTransformer, util # for embeddings\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches # Import for creating legend patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f1090-8666-4425-8df3-b8c8aa22c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "# Supress Hugging Face's Symlinks warning. It wants to use Symlinks to save disk space\n",
    "# But that requires you to turn on Windows developer mode, which comes with its own risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22f0eb-0caa-4191-b797-fefed485607d",
   "metadata": {},
   "source": [
    "**Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e40fdc-032c-4ce4-9fdb-8a30f0710d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad = banidb.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a66fb-69ff-4b32-8c36-9ed72df1f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45526bee-d7b9-4764-9b5d-e4a831f3c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbd271-45df-4d97-a032-443a4591a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shabad['shabad_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c083439-171c-4202-9ad8-df55e016d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shabad['source_uni'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095d1bc-d0d9-4586-8313-a257c6f97a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shabad['writer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e64fc-3284-4a30-af4b-ec53128465a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shabad['ang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc663164-2a98-4f78-bcf2-44c37d8466a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad['verses'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a2ca4-6544-4e8b-9c58-4304f8521dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad['verses'][1] # starting verse of the shabad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc0925-bd01-4606-8b9d-7f76bd54074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad['verses'][-1] # ending verse of the shabad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6e56d-0f6c-41b4-9f01-739dd6e71bc0",
   "metadata": {},
   "source": [
    "**Searching a shabad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3124a0b-25f3-463c-ad56-f15202dd96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gurbani_text = \"ਥਿਰੁ ਘਰਿ ਬੈਸਹੁ ਹਰਿ ਜਨ ਪਿਆਰੇ\" # Thir ghar baiso har jan piaare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a4b22-eaf7-469c-a790-3402b6dd0226",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Try searching 10 times across all sources, all angs, all angs, and all writers\n",
    "# The default search options for banidb.search() are:\n",
    "# banidb.search(query, searchtype=1, source='all', larivaar=False,\n",
    "#              ang=None, raag=None, writer='all', page=1, results=None)\n",
    "for i in range(10):\n",
    "    ascii_query = firstLetters(gurbani_text)\n",
    "    shabad_data = banidb.search(ascii_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594a811-62f9-44e2-bbf5-f5d71d730b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ascii_query)\n",
    "shabad_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8602b-ec7c-438b-80ff-496e39caa652",
   "metadata": {},
   "source": [
    "Now, let's find a shabad using a restricted search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050be5f9-5565-4251-8676-a61b2a702ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad_data['pages_data']['page_1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf54705-5d94-4c35-914a-9c7834db7f21",
   "metadata": {},
   "source": [
    "To restrict search space, the options in banidb.search are:\n",
    "1. source\n",
    "2. ang\n",
    "3. raag\n",
    "4. writer<br>\n",
    "Let's try to extract these from shabad_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ed71a-838f-44b9-a6c0-598c232c1fe8",
   "metadata": {},
   "source": [
    "*Source*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3b24d-8e2e-4d7c-98c0-16c731c366d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "banidb.sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f61478-6bb6-4a3c-921f-034dc271484b",
   "metadata": {},
   "source": [
    "I think that the search space can be universally restricted to B (I don't know how this is different from S), D, G, N from https://banidbpy.readthedocs.io/en/latest/sources.html but it seems that the search function only allows for a string i.e. a single source instead of multiple ones: https://banidbpy.readthedocs.io/en/latest/searchdb.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24cdbc-f7df-405e-8533-cf9512658c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# banidb.search() accepts source ID, not the source in english or unicode (https://banidbpy.readthedocs.io/en/latest/sources.html)\n",
    "# Map 'source_eng', which is what we get from banidb.search()'s output back to source ID so that we can use that in subsequent\n",
    "# calls to banidb.search()\n",
    "source_to_id_dict = {}\n",
    "for item in banidb.sources():\n",
    "    source_to_id_dict[item['source_eng']] = item['source_id']\n",
    "source_to_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa501fc-e24e-4007-b3b2-5e523bfadb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract source for the shabad\n",
    "shabad_data['pages_data']['page_1'][0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5ef26-a447-40a6-9a7a-9bbc44ea6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "shabad_source = shabad_data['pages_data']['page_1'][0]['source']['en']\n",
    "print(shabad_source)\n",
    "banidb.search(ascii_query, source = source_to_id_dict.get(shabad_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3009f4-ada3-49de-9c57-7cfb44dcff13",
   "metadata": {},
   "source": [
    "*Ang*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17ecba-bdbb-4095-9448-bf6df573aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad_ang = shabad_data['pages_data']['page_1'][0]['source']['ang']\n",
    "print(shabad_ang)\n",
    "# Sanity check\n",
    "banidb.search(ascii_query, ang = shabad_ang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143fa25-b414-4e74-846f-f60a18d95f50",
   "metadata": {},
   "source": [
    "*Raag*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0996a-5ad1-43fa-b7a8-45bd799ffbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raags = banidb.raags() # If this fails, try the one below\n",
    "except:\n",
    "    print(\"banidb.raags() doesn't work. Retrieving raags directly from the API\")\n",
    "    def get_raags_directly():\n",
    "        \"\"\"\n",
    "        Fetches the list of raags directly from the BaniDB API,\n",
    "        bypassing the banidb.raags() function.\n",
    "        \"\"\"\n",
    "        # The API endpoint that the banidb library uses for raags\n",
    "        url = \"https://api.banidb.com/v2/raags\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            # Raise an exception if the request returned an error (e.g., 404, 500)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Convert the JSON response to a Python dictionary\n",
    "            data = response.json()\n",
    "            \n",
    "            # The actual raag data is in the 'rows' key, skipping the header row\n",
    "            raags_list = []\n",
    "            for row in data['rows'][1:]:\n",
    "                raag = {\n",
    "                    'raag_id': row.get('RaagID'),\n",
    "                    'raag_uni': row.get('RaagUnicode'),\n",
    "                    'raag_eng': row.get('RaagEnglish')\n",
    "                }\n",
    "                raags_list.append(raag)\n",
    "                \n",
    "            return raags_list\n",
    "    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"A network error occurred: {e}\")\n",
    "            return None\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"The API response format may have changed. Error: {e}\")\n",
    "            return None\n",
    "\n",
    "        # --- Usage ---\n",
    "    raags = get_raags_directly()\n",
    "        \n",
    "if raags:\n",
    "    print(\"Successfully retrieved:\")\n",
    "    # Print the first 5 raags as an example\n",
    "    for raag in raags:\n",
    "        print(raag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97cddb-4e6c-4faf-8770-e5f6d696dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# banidb.search() accepts source ID, not the source in english or unicode (https://banidbpy.readthedocs.io/en/latest/sources.html)\n",
    "# Map 'raag_eng', which is what we get from banidb.search()'s output back to 'raag_id' so that we can use that in subsequent\n",
    "# calls to banidb.search()\n",
    "raag_to_id_dict = {}\n",
    "for item in raags: # or banidb.raags() if it doesn't fail\n",
    "    raag_to_id_dict[item['raag_eng']] = item['raag_id']\n",
    "raag_to_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b71c7-43e4-43dd-92ca-d558de9978cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad_raag = shabad_data['pages_data']['page_1'][0]['source']['raagen']\n",
    "print(shabad_raag)\n",
    "# Sanity check\n",
    "banidb.search(ascii_query, raag = raag_to_id_dict.get(shabad_raag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87f152-3f68-4c6a-a4d1-441d2f624a84",
   "metadata": {},
   "source": [
    "*Writer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e0fc3-f51f-4d3e-813b-289baef0b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "banidb.writers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6ce6e-271a-42c4-8a53-c326f1759443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# banidb.search() accepts writer ID, not the writer name or unicode (https://banidbpy.readthedocs.io/en/latest/writers.html)\n",
    "# Map 'writer_name', which is what we get from banidb.search()'s output back to 'writer_id' so that we can use that in subsequent\n",
    "# calls to banidb.search()\n",
    "writer_to_id_dict = {}\n",
    "for item in banidb.writers():\n",
    "    writer_to_id_dict[item['writer_name']] = item['writer_id']\n",
    "writer_to_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25004bf4-186d-4d1c-834b-2be228628456",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad_writer = shabad_data['pages_data']['page_1'][0]['source']['writer']\n",
    "print(shabad_writer)\n",
    "# Sanity check\n",
    "banidb.search(ascii_query, writer = writer_to_id_dict.get(shabad_writer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e0a87-a8d6-4307-ad6f-3b48ac58ab10",
   "metadata": {},
   "source": [
    "*Checking speed for restricted space search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ca12e-39bd-41b4-884e-70555aacbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad_source_id = source_to_id_dict.get(shabad_source)\n",
    "shabad_raag_id   = raag_to_id_dict.get(shabad_raag)\n",
    "shabad_writer_id = writer_to_id_dict.get(shabad_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39424021-5720-4c5a-9103-682d47a65e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    ascii_query = firstLetters(gurbani_text)\n",
    "    shabad_data = banidb.search(ascii_query, source=shabad_source_id, ang=shabad_ang, raag=shabad_raag_id, writer=shabad_writer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6685b-6d3c-4316-826c-3e0e2e2ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "banidb.search(ascii_query, source=shabad_source_id, ang=shabad_ang, raag=shabad_raag_id, writer=shabad_writer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbc1d2-b97a-402a-8085-5be2b8e09981",
   "metadata": {},
   "source": [
    "Wow. Seems like this didn't help at all. Also, while it is good to see how to improve the tuk/line retrieval time, at the end of the day, we don't even know if we can get ASR correct. If the ASR is incorrect, then the first letters would be incorrect too, and doing a banidb.search() would be futile no matter how optimized it is. Similarly, it is very common for ragis to repeat phrases e.g. \"satgur tumre kaaj savare, kaaj savare, kaaj savare\". This would also cause a banidb.search() to fail. So, a better approach is to take the ASR output and convert that into an embedding that can then be compared against the verses of the shabad. To do so, we first need to get all the verses of the shabad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e64f98-e799-4e5a-945f-b039d0c5db52",
   "metadata": {},
   "source": [
    "**Shabad verses and embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d4247-a054-48ff-b6d9-39673ea2c378",
   "metadata": {},
   "source": [
    "It is great that we can identify a shabad from across all Sikh scriptures. \n",
    "1. We have also found that restricting the search space in banidb.search() doesn't seem to improve the time required to identify.\n",
    "2. In addition, banidb would require us to have the correct search query to find a shabad. Since we will rely on ASR for getting the text for the gurbani tuk/line, we can't be certain of that. <br>\n",
    "\n",
    "So, we need to find a new method. Here is the proposed approach:\n",
    "1. Identify the shabad by using banidb search via skip-gram approach. That is, take the whole text sequence and split it into multiple parts. Then, pass this into banidb.search(). If the text sequence's first letters using ASR turn out to be \"tgvhjp\" (\"thir ghar baiso har jan piaare\"), where \"baiso\" is incorrectly identified as \"vaiso\", then we can pass multiple queries by splitting - \"tgvhjp\", \"gvhj\", \"tvhj\", \"vhjp\" etc. The return from these queries can be used to identify shabad IDs. The median of these results can be picked as the relevant shabad. We can also store other results in memory in case the search query fails in the future.\n",
    "2. Once the shabad is identified, we will get all its verses using banidb.shabad(). Then, we will no longer use the skip-gram approach for subsequent searches. Instead, we will use the more powerful method of embeddings. We will embed the ASR output as embeddings and compare them to the embeddings of the verses for that shabad. This search space should be quite small and we hope to be able to identify the verse fairly quickly.\n",
    "3. We need to continuously monitor the words being sung and ensure that the verse/tuk identified is same as the previous one. If it changes (with high confidence), we need to switch to giving a new verse as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115275c0-b857-4f71-9423-c1550b3384c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34cb89-4910-4cc3-9fb9-a50f3f92b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gurbani_verse(text):\n",
    "    \"\"\"\n",
    "    Removes verse numbers, punctuation, and the word 'Rahaao' from a Gurbani line.\n",
    "    This is needed because in ASR, we will never get these in the output as no raagi\n",
    "    sings these.\n",
    "    \"\"\"\n",
    "    # Remove numbers (both Gurmukhi and Arabic)\n",
    "    text = re.sub(r'[\\u0A66-\\u0A6F0-9]+', '', text)\n",
    "    # Remove the word 'Rahaao' (ਰਹਾਉ)\n",
    "    text = text.replace('ਰਹਾਉ', '')\n",
    "    # Remove Danda (॥), Visarg (ਃ), and other common punctuation\n",
    "    text = re.sub(r'[॥।☬ਃ|]', '', text)\n",
    "    # Remove extra spaces and strip whitespace from ends\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c23633-a2a8-4ba2-99fb-7e172341d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "shabad_id = shabad_data['pages_data']['page_1'][0]['shabad_id']\n",
    "shabad_data2 = banidb.shabad(shabad_id)\n",
    "[(clean_gurbani_verse(item['verse']), item['verse']) for item in shabad_data2['verses']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bb0bd-5419-4a32-8dd2-f6eff484069e",
   "metadata": {},
   "source": [
    "Using sentence-transformers/distiluse-base-multilingual-cased-v2 gives poor results. Same with ai4bharat/indic-bert' but that's because it is not a sentence transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4396ef-677e-4780-bd45-17b2f5781a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ShabadMatcher:\n",
    "    def __init__(self, model_name: str, pooling: str = 'cls', use_cls: bool = False):\n",
    "        self.model_name = model_name\n",
    "        self.pooling = pooling\n",
    "        self.use_cls = use_cls\n",
    "\n",
    "        if model_name == \"ai4bharat/indic-bert\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "            self.encode_fn = self._encode_indic_bert\n",
    "\n",
    "        elif model_name == \"bert-base-multilingual-cased\":\n",
    "            word_embedding_model = models.Transformer(model_name, max_seq_length=128)\n",
    "            pooling_model = models.Pooling(\n",
    "                word_embedding_model.get_word_embedding_dimension(),\n",
    "                pooling_mode_mean_tokens=(pooling == 'mean'),\n",
    "                pooling_mode_cls_token=(pooling == 'cls'),\n",
    "                pooling_mode_max_tokens=False\n",
    "            )\n",
    "            self.model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "            self.encode_fn = self._encode_sbert\n",
    "\n",
    "        elif model_name in [\n",
    "            \"l3cube-pune/indic-sentence-similarity-sbert\",\n",
    "            \"sentence-transformers/LaBSE\",\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        ]:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.encode_fn = self._encode_sbert\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    def _encode_sbert(self, texts):\n",
    "        return self.model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    def _encode_indic_bert(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        if self.use_cls:\n",
    "            embeddings = outputs.last_hidden_state[:, 0]\n",
    "        else:\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size())\n",
    "            masked_embeddings = outputs.last_hidden_state * mask\n",
    "            summed = masked_embeddings.sum(dim=1)\n",
    "            counts = mask.sum(dim=1)\n",
    "            embeddings = summed / counts\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    def match(self, shabad_data, asr_output_phrase, min_words=3):\n",
    "        if len(asr_output_phrase.strip().split()) < min_words:\n",
    "            print(f\"[Skipping] Phrase too short (min_words={min_words}): '{asr_output_phrase}'\")\n",
    "            return None\n",
    "            \n",
    "        verses = [clean_gurbani_verse(item['verse']) for item in shabad_data['verses']]\n",
    "        shabad_embeddings = self.encode_fn(verses)\n",
    "        asr_embedding = self.encode_fn(asr_output_phrase)\n",
    "\n",
    "        if isinstance(shabad_embeddings, torch.Tensor):\n",
    "            scores = F.cosine_similarity(asr_embedding, shabad_embeddings)\n",
    "            best_idx = scores.argmax()\n",
    "            best_score = scores[best_idx]\n",
    "        else:\n",
    "            scores = util.cos_sim(asr_embedding, shabad_embeddings)\n",
    "            best_idx = scores.argmax()\n",
    "            best_score = scores[0][best_idx]\n",
    "\n",
    "        return {\n",
    "            'best_verse': verses[best_idx],\n",
    "            'score': round(float(best_score), 2),\n",
    "            'index': int(best_idx)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14144ff4-ea08-4336-abb8-31937be10609",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = {\n",
    "    \"BBMC\": ShabadMatcher(\"bert-base-multilingual-cased\", pooling=\"cls\"),\n",
    "    \"L3Cube\": ShabadMatcher(\"l3cube-pune/indic-sentence-similarity-sbert\"),\n",
    "    \"LaBSE\": ShabadMatcher(\"sentence-transformers/LaBSE\"),\n",
    "    \"MiniLM\": ShabadMatcher(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"),\n",
    "    \"IndicBERT (CLS)\": ShabadMatcher(\"ai4bharat/indic-bert\", use_cls=True),\n",
    "    \"IndicBERT (Mean)\": ShabadMatcher(\"ai4bharat/indic-bert\", use_cls=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f030e34-c188-419c-85cc-994a6a004e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_phrases(asr_phrases, correct_indices):\n",
    "    results_verse =  {key:[] for key in models_to_test.keys()}\n",
    "    results_status =  {key:[] for key in models_to_test.keys()}\n",
    "    results_score =  {key:[] for key in models_to_test.keys()}\n",
    "    assert(len(asr_phrases) == len(correct_indices))\n",
    "    for i, phrase in enumerate(asr_phrases):\n",
    "        print(f\"\\n------For {phrase}------\")\n",
    "        for name, matcher in models_to_test.items():\n",
    "            result = matcher.match(shabad_data2, phrase)\n",
    "            status = False if result is None else int(result['index'] == correct_indices[i])\n",
    "            results_status[name].append(status)\n",
    "            verse = None if result is None else result['best_verse']\n",
    "            results_verse[name].append(verse)\n",
    "            score = None if result is None else result['score']\n",
    "            results_score[name].append(score)\n",
    "            print(f\"{name}: {result}. Correct? {bool(status) if status is not None else status}\")\n",
    "    return results_status, results_verse, results_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a975c9-8ef3-43b2-9260-cf05bf3b64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_by_status(models_to_test, statuses, scores):\n",
    "    \"\"\"\n",
    "    Creates a 3x2 grid of bar plots showing similarity scores,\n",
    "    grouped and sorted by status, with a clear legend.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 13))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, model in enumerate(models_to_test):\n",
    "        status_list = statuses.get(model, [])\n",
    "        score_list = scores.get(model, [])\n",
    "\n",
    "        # Separate data by status\n",
    "        status_1_data = sorted(\n",
    "            [(score_list[j], f'Inst {j}') for j, s in enumerate(status_list) if s == 1],\n",
    "            key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        status_0_data = sorted(\n",
    "            [(score_list[j], f'Inst {j}') for j, s in enumerate(status_list) if s == 0],\n",
    "            key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        \n",
    "        # Unzip sorted data, handling cases where a status might be missing\n",
    "        scores_1, labels_1 = zip(*status_1_data) if status_1_data else ([], [])\n",
    "        scores_0, labels_0 = zip(*status_0_data) if status_0_data else ([], [])\n",
    "\n",
    "        # Combine for plotting\n",
    "        combined_scores = list(scores_1) + list(scores_0)\n",
    "        combined_labels = list(labels_1) + list(labels_0)\n",
    "        combined_colors = ['tab:blue'] * len(scores_1) + ['tab:orange'] * len(scores_0)\n",
    "        \n",
    "        ax = axs[i]\n",
    "        if combined_scores:\n",
    "            ax.bar(range(len(combined_scores)), combined_scores, color=combined_colors)\n",
    "        \n",
    "        ax.set_title(model)\n",
    "        ax.set_xlabel('Instance')\n",
    "        ax.set_ylabel('Similarity Score')\n",
    "        ax.set_ylim(np.min(combined_scores)-0.01, np.max(combined_scores)+0.01)\n",
    "        ax.set_xticks(range(len(combined_labels)))\n",
    "        ax.set_xticklabels(combined_labels, rotation=45, ha='right')\n",
    "\n",
    "    # Create legend handles and add them to the figure just once\n",
    "    legend_patch_1 = mpatches.Patch(color='tab:blue', label='Correct verse')\n",
    "    legend_patch_0 = mpatches.Patch(color='tab:orange', label='Incorrect verse')\n",
    "    fig.legend(handles=[legend_patch_1, legend_patch_0], loc='upper right')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to make space for the legend\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60754de-da4f-4cc0-9175-334d95c2f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_asr_phrases = [\n",
    "    'ਦੁਸਟ', 'ਦੁਸ਼ਟ', 'ਦੁਸਟ ਦੂਤ', 'ਕਰ ਦੀਨੇ', 'ਕਰਤਾਰੇ', 'ਕੀਨੋ ਦਾਨ',\n",
    "    'ਦਾਨ', 'ਨਿਰਭੌ', 'ਸਾਧ ਸੰਗ', 'ਸਾਧ', 'ਅੰਤਰ', 'ਜਾਮੀ', 'ਪਕੜੀ ਪ੍ਰਭ'\n",
    "]\n",
    "short_asr_correct = [3, 3, 3, 5, 4, 8,\n",
    "                     8, 7, 8, 8, 9, 9, 10]\n",
    "statuses, verses, scores = test_phrases(short_asr_phrases, short_asr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4d8d2-4501-44ea-a753-31f640d90e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 word phrases, all beginning and ending within the same verse (ideal use case)\n",
    "long_asr_phrases = [\n",
    "    'ਸਤਗੁਰ ਤੁਮਰੇ ਕਾਜ', 'ਤੁਮਰੇ ਕਾਜ ਸਵਾਰੇ', 'ਦੁਸ਼ਟ ਦੂਤ ਪਰਮੇਸਰ', 'ਦੂਤ ਪਰਮੇਸਰ ਮਾਰੇ', 'ਜਨ ਕੀ ਪੈਜ', 'ਕੀ ਪੈਜ ਰਖੀ', 'ਪੈਜ ਰਖੀ ਕਰਤਾਰੇ',\n",
    "    'ਨਿਰਭੌ ਹੋ ਭਜੋ', 'ਹੋ ਭਜੋ ਭਗਵਾਨ', 'ਹੋ ਭਜੋ ਪਗਵਾਨ', 'ਬਾਦ ਸ਼ਾਹ ਸ਼ਾਹ', 'ਸ਼ਾਹ ਸ਼ਾਹ ਸਬ', 'ਵੱਸ ਕਰ ਦੀਨੇ', 'ਅੰਮ੍ਰਿਤ ਨਾਮ ਮਹਾ']\n",
    "long_asr_correct = [2, 2, 3, 3, 4, 4, 4,\n",
    "                       7, 7, 7, 5, 5, 5, 6]\n",
    "    \n",
    "long_statuses, long_verses, long_scores = test_phrases(long_asr_phrases, long_asr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400c60d-47e0-4c17-8a89-df76343bdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity_by_status(models_to_test, long_statuses, long_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024cf71-3d0f-4f8f-bbc5-4e28621dcff0",
   "metadata": {},
   "source": [
    "0 'ਗਉੜੀ ਮਹਲਾ',\n",
    "1 'ਥਿਰੁ ਘਰਿ ਬੈਸਹੁ ਹਰਿ ਜਨ ਪਿਆਰੇ',\n",
    "2 'ਸਤਿਗੁਰਿ ਤੁਮਰੇ ਕਾਜ ਸਵਾਰੇ',\n",
    "3 'ਦੁਸਟ ਦੂਤ ਪਰਮੇਸਰਿ ਮਾਰੇ',\n",
    "4 'ਜਨ ਕੀ ਪੈਜ ਰਖੀ ਕਰਤਾਰੇ',\n",
    "5 'ਬਾਦਿਸਾਹ ਸਾਹ ਸਭ ਵਸਿ ਕਰਿ ਦੀਨੇ',\n",
    "6 'ਅੰਮ੍ਰਿਤ ਨਾਮ ਮਹਾ ਰਸ ਪੀਨੇ',\n",
    "7 'ਨਿਰਭਉ ਹੋਇ ਭਜਹੁ ਭਗਵਾਨ',\n",
    "8 'ਸਾਧਸੰਗਤਿ ਮਿਲਿ ਕੀਨੋ ਦਾਨੁ',\n",
    "9 'ਸਰਣਿ ਪਰੇ ਪ੍ਰਭ ਅੰਤਰਜਾਮੀ',\n",
    "10 'ਨਾਨਕ ਓਟ ਪਕਰੀ ਪ੍ਰਭ ਸੁਆਮੀ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254a1b0-c878-4092-8f7a-c05b18311fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 word phrases, beginning in one verse and ending in another (edge case)\n",
    "mixed_asr_phrases = [\n",
    "    'ਜਨ ਪਿਆਰੇ ਸਤਗੁਰ', 'ਪਿਆਰੇ ਸਤਗੁਰ ਤੁਮਰੇ', 'ਕਾਜ ਸਵਾਰੇ ਦੁਸ਼ਟ', 'ਸਵਾਰੇ ਦੁਸ਼ਟ ਦੂਤ', 'ਪਰਮੇਸਰ ਮਾਰੇ ਜਨ', \n",
    "    'ਮਾਰੇ ਜਨ ਕੀ',  'ਰਖੀ ਕਰਤਾਰੇ ਬਾਦ', 'ਪੀਨੇ ਨਿਰਭੌ ਹੋ', 'ਭਜੋ ਪਗਵਾਨ ਸਾਧ', 'ਕੀਨੋ ਦਾਨ ਸਰਣ', 'ਦਾਨ ਸਰਣ ਪਰੇ', 'ਦਾਨ ਸਰਣ ਭਰੇ']\n",
    "# detect the correct phrase/index as the one with the dominant number of words\n",
    "mixed_asr_correct = [1, 2, 2, 3, 3, \n",
    "                     4, 4, 7, 7, 8, 9, 9]\n",
    "mixed_statuses, mixed_verses, mixed_scores = test_phrases(mixed_asr_phrases, mixed_asr_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76c467-b91c-4e1d-ab8a-0cdd45475f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity_by_status(models_to_test, mixed_statuses, mixed_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf7f55-6c6b-48a3-9ba2-234e87096752",
   "metadata": {},
   "outputs": [],
   "source": [
    "shabad_data['pages_data']['page_1'][0]['shabad_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e64dda-23d7-469c-b08d-05e2f60fafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# 1. Add logic so that if all the words of a verse have not been spoken, we will not switch to the next verse. This might be tricky because ragis could, \n",
    "# in principle, just sing a few words of a tuk and switch to another. Usually, these would only be the ending words of a verse i.e. they might sing\n",
    "# 'kaaj savare' multiple times. But it is unlikely that they will sing the first few words of a verse multiple times without saying the next ones\n",
    "# in the verse.\n",
    "# 2. The 3-word method will fail when very similar lines are being spoken. For example, in Sukhmani Sahib, there are multiple lines starting with\n",
    "#    'prabh ke simran'. Similarly, in Bhai Nand Lal's vaaran, there are repeated lines like \n",
    "#     'Nasro Mansoor Gur Gobind Singh,... Hak hak aaena Gur Gobind Singh' etc. where 'Gur Gobind Singh' is repeated. In such instances, picking \n",
    "#      one verse with the highest cosine similarity will fail. To fix this, we have to have a logic along the lines of:\n",
    "#    a) Find cosine similarity with all verses\n",
    "#    b) Pick the one with the highest similarity, provided that the cosine similarity of the highest one is above a certain threshold AND the difference\n",
    "#       between the highest and second highest is more than a separate threshold. If both of these conditions aren't satisfied, wait to hear more words\n",
    "#       and try verse matching with a larger dataset.\n",
    "# Make the initial shabad identification method more robust if need be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0094e-f2d8-4cb8-8eae-59d963964d98",
   "metadata": {},
   "source": [
    "**Automatic Speech Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f8df5-2c16-4514-8344-adf4959411cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Try https://huggingface.co/gagan3012/wav2vec2-xlsr-punjabi\n",
    "# Need to search more for other models. Google speech to text is amazing and we can leverage their $300 free credits as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gurbani_ai)",
   "language": "python",
   "name": "gurbani_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
